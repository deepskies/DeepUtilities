{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Diagnostics.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "87ZbM5a21vnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "import seaborn as sn\n",
        "import numpy as np\n",
        "from matplotlib.pyplot import cm\n",
        "import scikitplot as skplt\n",
        "import argparse\n",
        "\n",
        "class Diagnostics(self, actual, predicted, acc=0, loss=0, pixel_values=0, feature_list=0, cross_val=0, auc=0,):\n",
        "  \n",
        "  # Mandatory lists for diagnostics\n",
        "  self.actual=actual; self.predicted=predicted \n",
        "  \n",
        "  # User-added lists for diagnostics\n",
        "  self.acc=acc; self.loss=loss; self.auc = auc; self.pixel_values=pixel_values; self.feature_lists=feature_list; self.cross_val=cross_val\n",
        "  \n",
        "\n",
        "  # Plot axes formatting\n",
        "  plt.rcParams['axes.linewidth']=3\n",
        "  plt.rcParams['xtick.major.width'] = 2\n",
        "  plt.rcParams['ytick.major.width'] = 2\n",
        "  plt.rcParams['xtick.minor.width'] = 2\n",
        "  plt.rcParams['ytick.minor.width'] = 2\n",
        "  plt.rc('xtick.major', size=8, pad=8)\n",
        "  plt.rc('xtick.minor', size=6, pad=5)\n",
        "  plt.rc('ytick.major', size=8, pad=8)\n",
        "  plt.rc('ytick.minor', size=6, pad=5)\n",
        "  \n",
        "  \n",
        "  # Plots confusion matrix. If norm is set, values are between 0-1. Shows figure if show is set\n",
        "  def plot_cm(figsize = (6, 4), norm=True, show=True):\n",
        "    \"\"\"\n",
        "    Creates a confusion matrix for the predicted and actual labels for your model\n",
        "    \n",
        "    Input:\n",
        "       - figsize: tuple, the figure size of the desired plot\n",
        "       - norm: boolean, whether or not you want your confusion matrix normalized (between 0-1)\n",
        "       - show: boolean, whether you want to plt.show() your figure or just save it to your computer \n",
        "    \"\"\"\n",
        "    cm=confusion_matrix(self.actual, self.predicted)\n",
        "    plt.figure(figsize=figsize)\n",
        "    labels = np.unique(self.predicted).tolist()\n",
        "    if (norm):\n",
        "      heatmap_value = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "      file_name = \"Confusion_Matrix_Norm.jpeg\"\n",
        "    else:\n",
        "      heatmap_value = cm.astype('float')\n",
        "      file_name = \"Confusion_Matrix.jpeg\"\n",
        "    sn.heatmap(heatmap_value, annot=True, xticklabels=labels, yticklabels=labels, cmap=\"Blues\", annot_kws={\"size\": 14})\n",
        "    \n",
        "    if (norm):\n",
        "      plt.title(\"Normalized Confusion Matrix\", fontsize=14)\n",
        "    else:\n",
        "      plt.title(\"Confusion Matrix\", fontsize=14)\n",
        "      \n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.ylabel(\"True Label\", fontsize=14)\n",
        "    plt.xlabel(\"Predicted Label\", fontsize=14)\n",
        "    \n",
        "    plt.savefig(file_name)\n",
        "    if (show): plt.show()\n",
        "    plt.close()\n",
        "    np.set_printoptions(precision=2)\n",
        "    \n",
        "  \n",
        "  # Plots metrics by epoch. Plots either \"loss\" or \"accuracy\" based on keyword (default is both). Shows figure if show is set\n",
        "  def plot_metrics_per_epoch(figsize = (6, 4), name_plot=(0,1,2), show=True):\n",
        "      \"\"\"\n",
        "      Plots accuracy, loss, and auc curves per epoch\n",
        "      \n",
        "      Input: \n",
        "        - figsize: tuple, the size of the metric curve\n",
        "        - name_plot: tuple, whether you want '0' (loss), '1' (accuracy), and/or '2' (auc) plots\n",
        "        - show: boolean, whether you want to plt.show() your figure or just save it to your computer \n",
        "      \"\"\"\n",
        "      num_graphs = len(name_plot)\n",
        "      fig, axes = plt.subplots(nrows=1, ncols=num_graphs, figsize=figsize)\n",
        "      format_plot_axes()\n",
        "      \n",
        "      for i,ele in enumerate(name_plot):\n",
        "        if (ele == 0):\n",
        "          metric_epoch_train = self.loss[0]\n",
        "          metric_epoch_valid = self.loss[1]\n",
        "          name_plot = \"Loss\"\n",
        "        elif (ele == 1):\n",
        "          metric_epoch_train = self.acc[0]\n",
        "          metric_epoch_valid = self.acc[1]\n",
        "          name_plot = \"Accuracy\"\n",
        "        elif (ele == 2):\n",
        "          metric_epoch_train = self.auc[0]\n",
        "          metric_epoch_valid = self.auc[1]\n",
        "          name_plot = \"AUC\"\n",
        "        else:\n",
        "          print(\"Improper value inputted, ignoring value\")\n",
        "          break\n",
        "                  \n",
        "        axes[i].plot(metric_epoch_train, '-', color='seagreen', label='Training')\n",
        "        axes[i].plot(metric_epoch_valid, '--', color='blue', label='Validation')\n",
        "        fig.title(\"Epoch vs \" + name_plot, fontsize=26)\n",
        "\n",
        "      fig.xlabel(\"Epoch\", fontsize=20)\n",
        "      fig.ylabel(name_plot, fontsize=20)\n",
        "      fig.xticks(fontsize=16); plt.yticks(fontsize=16)\n",
        "      fig.legend(loc='best')\n",
        "\n",
        "      file_name = title.replace(\" \", \"_\") + \".jpeg\"\n",
        "      \n",
        "      extent = axes[i].get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
        "\n",
        "      fig.savefig(file_name, bbox_inches='tight', transparent=True, bbox_inches=extent)\n",
        "      \n",
        "    if (show): fig.show()      \n",
        "    fig.close()\n",
        "      \n",
        "    \n",
        "    \n",
        "  def plot_cross_validation(figsize = (6, 4), show=True):\n",
        "    file_name = \"K_fold_Cross_Validation.jpeg\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.tile(\"K-fold Cross Validation\", fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.ylabel(\"Folds\", fontsize=14)\n",
        "    plt.xlabel(\"Accuracy\", fontsize=14)\n",
        "    plt.plot(self.loss)\n",
        "    plt.savefig(file_name)\n",
        "    if (show): plt.show()\n",
        "    plt.close()\n",
        "    \n",
        "    \n",
        "  def ROC_plot_sk(figsize = (6, 4), show=True):\n",
        "    \"\"\"\n",
        "    Plots the ROC curve between the predicted and actual labels\n",
        "    Note: \"actual\" labels must be a 1D array\n",
        "    \"\"\"\n",
        "    skplt.metrics.plot_roc(self.actual, self.prediction, figsize=figsize)\n",
        "    if (show): plt.show()\n",
        "    plt.close()\n",
        "    \n",
        "    \n",
        "  def ROC_plot(figsize = (6, 4), show=True):\n",
        "    true_positive = numpy.count_nonzeros(self.actual*self.predicted) # people are actually positive that you declare positive\n",
        "    false_negative = numpy.count_nonzeros(self.predicted*numpy.where(self.actual == 1, 0, 1)) # people are actually negative that you declare positive\n",
        "    true_positive_rate = true_positive/(true_positive+false_negative) \n",
        "    true_negatve = numpy.count_nonzero(self.predicted)-true_positive # people are actually negative that you declare negative\n",
        "    false_positive = numpy.count_nonzeros(numpy.where(self.predicted == 1, 0, 1) - false_negative # people are actually positive that you declare negative\n",
        "    false_positive_rate = 1 - (true_negative/(true_negative+false_positive))\n",
        "                                          \n",
        "                                          \n",
        "    file_name = \"ROC.jpeg\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.tile(\"ROC\", fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.ylabel(\"TPR\", fontsize=14)\n",
        "    plt.xlabel(\"FPR\", fontsize=14)\n",
        "    plt.plot(false_positive_rate, true_positive_rate)\n",
        "    plt.savefig(file_name)\n",
        "    if (show): plt.show()\n",
        "    plt.close()\n",
        "                                              \n",
        "    \n",
        "  def residual_dist_by_feature(figsize = (6,8), target='Target', hex_bin=False, show=True):\n",
        "   file_name = '{}_errors_by_feature.pdf'.format(target)\n",
        "   #Calculate residuals as fractional error.\n",
        "   error=2*(self.predicted-self.actual)/(abs(self.actual)+abs(self.predicted))\n",
        "   num_features=len(self.feature_list.columns); figure_width, figure_height = figsize\n",
        "   fig=plt.figure(figsize=(figure_width, figure_height*num_features))\n",
        "   for i in range(0, num_features):\n",
        "       ax = fig.add_subplot(num_features, 1, i+1)\n",
        "       #Plot the errors vs. feature.\n",
        "       if hex_bin==True:\n",
        "           ax.hexbin(feature_list[feature_list.columns[i]],error, bins='log')\n",
        "       else:\n",
        "           ax.plot(feature_list[feature_list.columns[i]],error, '.', alpha=0.2)\n",
        "       ax.set_xlabel(feature_list.columns[i], fontsize=14)\n",
        "       ax.set_ylabel('Fractional Error', fontsize=14)\n",
        "       plt.rc('xtick',labelsize=14)\n",
        "       plt.rc('ytick',labelsize=14)\n",
        "       ax.set_title('Fractional Error as a function of {}'.format(feature_list.columns[i]), fontsize=14)\n",
        "       extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
        "       plt.savefig(file_name, bbox_inches=extent)\n",
        "   if (show): fig.show()\n",
        "    \n",
        "  \n",
        "    \n",
        "  def one_to_one_plot(target_name='Target', axis_scale='linear', show=True):\n",
        "    file_name = '{}_One_to_One.pdf'.format(target_name)\n",
        "    plt.plot(self.actual, self.predicted, '.')\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.xlabel('True {}'.format(target_name), fontsize=14)\n",
        "    plt.ylabel('Predicted {}'.format(target_name), fontsize=14)\n",
        "    plt.title('One to one plot showing predicted vs. true {}'.format(target_name), fontsize=14)\n",
        "    plt.xscale(axis_scale)\n",
        "    plt.yscale(axis_scale)\n",
        "    line_x, line_y = np.arange(min(self.actual),1.1*max(self.actual),(max(self.actual)-min(self.actual))/10), np.arange(min(self.actual),1.1*max(self.actual),(max(self.actual)-min(self.actual))/10)\n",
        "    plt.plot(line_x,line_y,'r--')\n",
        "    plt.savefig(file_name)\n",
        "    if (show): plt.show()\n",
        "    plt.close()\n",
        "    \n",
        "    \n",
        "  def target_distributions(target='Target', x_scale='linear', y_scale='linear', show=True):\n",
        "    file_name = '{}_distributions.pdf'.format(target)\n",
        "    # Assign colors for each group and the names\n",
        "    colors = ['#E69F00', '#56B4E9']\n",
        "    names = ['True {}'.format(target), 'Predicted {}'.format(target)]\n",
        "    plt.hist([self.actual,self.predicted], bins = 50, color=colors, label=names)\n",
        "    plt.yscale(y_scale)\n",
        "    plt.xscale(x_scale)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.xticks(fontsize=14)\n",
        "    # Plot formatting\n",
        "    plt.legend()\n",
        "    plt.xlabel(target, fontsize=14)\n",
        "    plt.title('{} distributions for True and Predicted'.format(target), fontsize=14)\n",
        "    plt.savefig(file_name)\n",
        "    if (show): plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "  def plot_sample_img(data, labels, figsize, filename=\"Image_Sample.png\", show=True):\n",
        "    \"\"\"\n",
        "    Plots data where each row consists of the same image in different bands\n",
        "\n",
        "    Input:\n",
        "      data - an array of shape [batch_size, channels, height, width] OR [batch_size, height, width, channels]\n",
        "      labels - a 1D array of labels that match to the corresponding label\n",
        "      figsize - the figure size of the main plot\n",
        "      filename - saved filename\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    #plt.subplots_adjust(hspace=0.35)\n",
        "\n",
        "    counter = 1\n",
        "    num_imgs = len(data)\n",
        "\n",
        "    # if the image data is in the format [batch_size, channels, height, width]\n",
        "    if (data.shape)[1] < (data.shape)[3]:\n",
        "      num_bands = (data.shape)[1]\n",
        "      for i in range(len(data)):\n",
        "        for j in range(num_bands):\n",
        "          plt.subplot(num_imgs, num_bands, counter)  \n",
        "          plt.imshow(data[i][j], cmap='gray')\n",
        "          plt.title(\"Label: \"+ str(labels[i]), fontsize=14)\n",
        "          counter += 1\n",
        "\n",
        "    # if the image data is in the format [batch_size, height, width, channels]\n",
        "    else:\n",
        "      num_bands = (data.shape)[3]\n",
        "      for i in range(len(data)):\n",
        "        for j in range(num_bands):\n",
        "          plt.subplot(num_imgs, num_bands, counter)\n",
        "          plt.imshow(data[i, :, :, j], cmap='gray')\n",
        "          plt.title(\"Label:\"+ str(labels[i]), fontsize=14)\n",
        "          counter +=1\n",
        "\n",
        "    plt.savefig(filename)\n",
        "    if (show): plt.show()\n",
        "                                          \n",
        "                                          \n",
        "   def output_average_precision():\n",
        "    average_precision = average_precision_score(self.actual, self.predicted)\n",
        "    print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))\n",
        "                                          \n",
        "   \n",
        "   def precision_recall_plot(show=True):\n",
        "     preicision, recall = precision_recall_curve(self.actual, self.predicted)\n",
        "     tep_kwargs = ({'step': 'post'}\n",
        "               if 'step' in signature(plt.fill_between).parameters\n",
        "               else {})\n",
        "     plt.step(recall, precision, color='b', alpha=0.2,\n",
        "         where='post')\n",
        "     plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
        "\n",
        "     plt.xlabel('Recall')\n",
        "     plt.ylabel('Precision')\n",
        "     plt.ylim([0.0, 1.05])\n",
        "     plt.xlim([0.0, 1.0])\n",
        "     plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
        "          average_precision))\n",
        "     if (show): plt.show(); plt.close()\n",
        "                                          \n",
        "   def run_diagnostics(show = True):\n",
        "                                          \n",
        "\n",
        "\n",
        "\n",
        "           \n",
        "   \n",
        "    \n",
        "    \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}