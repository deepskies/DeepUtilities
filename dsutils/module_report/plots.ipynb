{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate report page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mosaic(imgs, nrows, ncols, border=1):\n",
    "    \"\"\"Make a nice mosaic.\n",
    "    \n",
    "    Given a set of images with all the same shape, makes a\n",
    "    mosaic with nrows and ncols;\n",
    "    intended for use with activation layer\n",
    "    \"\"\"\n",
    "    # set up data\n",
    "    nimgs = imgs.shape[0]\n",
    "    imshape = imgs.shape[1:]\n",
    "\n",
    "    mosaic = np.ma.masked_all((nrows * imshape[0] + (nrows - 1) * border,\n",
    "                           ncols * imshape[1] + (ncols - 1) * border),\n",
    "                           dtype=np.float32)\n",
    "\n",
    "    paddedh = imshape[0] + border\n",
    "    paddedw = imshape[1] + border\n",
    "    for i in xrange(nimgs):\n",
    "        row = int(np.floor(i / ncols))\n",
    "        col = i % ncols\n",
    "\n",
    "        mosaic[row * paddedh:row * paddedh + imshape[0],\n",
    "               col * paddedw:col * paddedw + imshape[1]] = imgs[i]\n",
    "    return mosaic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN architecture vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I did this in latex..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss/Accuracy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MINE\n",
    "def loss_acc_plot(loss, val_loss, acc, val_acc, epochs):\n",
    "# ------------------------------------------------------------------------------\n",
    "# Funciton plots a combined loss and accuracy plot for training and validation set\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "    figsize=(6,4)\n",
    "    fig, axis1 = plt.subplots(figsize=figsize)\n",
    "    plot1_lacc = axis1.plot(epochs, acc, 'navy', label='accuracy')\n",
    "    plot1_val_lacc = axis1.plot(epochs, val_acc, 'deepskyblue', label=\"validation accuracy\")\n",
    "\n",
    "    plot1_loss = axis1.plot(epochs, loss, 'red', label='loss')\n",
    "    plot1_val_loss = axis1.plot(epochs, val_loss, 'lightsalmon', label=\"validation loss\")\n",
    "\n",
    "\n",
    "    plots = plot1_loss + plot1_val_loss\n",
    "    labs = [l.get_label() for l in plots]\n",
    "    axis1.set_xlabel('Epoch')\n",
    "    axis1.set_ylabel('Loss/Accuracy')\n",
    "    plt.tight_layout()\n",
    "    axis1.legend(loc='center right')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix plot with original and normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MINE\n",
    "def CM_ROC(x_test,y_test):\n",
    "# ------------------------------------------------------------------------------\n",
    "# Evaluate classificaiton\n",
    "# Outputs the confusion matrix and ROC curve vith its AUC \n",
    "# ------------------------------------------------------------------------------\n",
    "    # predict\n",
    "    prob = loaded_model.predict(x_test)\n",
    "    pred =  (prob > 0.5).astype('int32') \n",
    "\n",
    "    # measure confusion\n",
    "    labels=[0, 1]\n",
    "    cm = metrics.confusion_matrix(y_test, pred, labels=labels)\n",
    "    cm = cm.astype('float')\n",
    "    cm_norm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"cm\", cm)\n",
    "    print(\"cm_norm\", cm_norm)\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, prob, pos_label=1)\n",
    "    auc = metrics.roc_auc_score(y_test, prob)\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "    #plotting\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion matrix (Pristine Images)',y=1.08)\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i in range(cm_norm.shape[0]):\n",
    "        for j in range(cm_norm.shape[1]):\n",
    "            ax.text(j, i, format(cm_norm[i, j], fmt),\n",
    "            ha=\"center\", va=\"center\",\n",
    "            color=\"white\" if cm_norm[i, j] < thresh else \"black\")\n",
    "    #pl.savefig('FINAL_SB00_Conf_Matrix_TEST_68_4_9.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    #ROC\n",
    "    figsize=(5,5)\n",
    "    fig, axis1 = plt.subplots(figsize=figsize)\n",
    "    x_onetoone = y_onetoone = [0, 1]\n",
    "\n",
    "    plt.plot(fpr, tpr, 'r-')\n",
    "    plt.plot(x_onetoone, y_onetoone, 'k--',  label=\"1-1\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.title(\"Receiver Operator Characteristic (ROC)\")\n",
    "    plt.xlabel(\"False Positive (1 - Specificity)\")\n",
    "    plt.ylabel(\"True Positive (Selectivity)\")\n",
    "    plt.tight_layout()\n",
    "    #pl.savefig('FINAL_SB00_ROC_TEST_bestweights_68_4_9.pdf')\n",
    "    return prob, pred, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(dir_analysis_figure, cm, label_quadrant=True, labels=[0,1]):\n",
    "    \"\"\"Plot the confusion matrix.\"\"\"\n",
    "    # check shape\n",
    "    assert cm.shape == (2,2), \"Not the right shape for input cm\"\n",
    "\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_style(\"ticks\")\n",
    "    n_colors=300\n",
    "    # cmap = sns.cubehelix_palette(n_colors, start=2.8, rot=.1, as_cmap=True) #, start=1, rot=1.2, as_cmap=True)\n",
    "    cmap = sns.color_palette(\"coolwarm\", n_colors=n_colors)#, color_codes=True)\n",
    "    cmap = ListedColormap(cmap)\n",
    "\n",
    "    # plot\n",
    "    plt.figure()\n",
    "    plt.imshow(np.transpose(cm[::-1,::-1]), \n",
    "               interpolation='nearest', cmap=cmap, extent=[1.5,-0.5,-0.5,1.5])\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel('Predicted', fontsize=20)\n",
    "    plt.ylabel('True', fontsize=20)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.ax.tick_params(labelsize=20) \n",
    "    plt.tight_layout()\n",
    "\n",
    "    if label_quadrant:\n",
    "        tp_x, tp_y = 1, 1\n",
    "        tn_x, tn_y = 0, 0\n",
    "        fn_x, fn_y = 0, 1\n",
    "        fp_x, fp_y = 1, 0\n",
    "\n",
    "        verticalalignment = 'center'\n",
    "        myformat = '{:04.3f}'\n",
    "        tp_str = \"TP:\\n\" + myformat.format(cm[tp_y,tp_x])\n",
    "        tn_str = \"TN:\\n\" + myformat.format(cm[tn_y,tn_x])\n",
    "        fp_str = \"FP:\\n\" + myformat.format(cm[fp_y,fp_x])\n",
    "        fn_str = \"FN:\\n\" + myformat.format(cm[fn_y,fn_x])\n",
    "        plt.text(tn_x, tn_y, tn_str,\n",
    "                 fontsize=25, color='white', fontweight='bold',\n",
    "                 verticalalignment=verticalalignment, \n",
    "                 horizontalalignment='center')\n",
    "        plt.text(tp_x, tp_y, tp_str,\n",
    "                 fontsize=25, color='white', fontweight='bold',\n",
    "                 verticalalignment=verticalalignment,\n",
    "                 horizontalalignment='center')\n",
    "        plt.text(fp_x, fp_y, fp_str,\n",
    "                 fontsize=25, color='white', fontweight='bold',\n",
    "                 verticalalignment=verticalalignment,\n",
    "                 horizontalalignment='center')\n",
    "        plt.text(fn_x, fn_y, fn_str,\n",
    "                 fontsize=25, color='white', fontweight='bold',\n",
    "                 verticalalignment=verticalalignment,\n",
    "                 horizontalalignment='center')\n",
    "\n",
    "    plt.tick_params(\n",
    "        axis='x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        top='off',\n",
    "        labelsize=20)         # ticks along the top edge are off\n",
    "    plt.tick_params(\n",
    "        axis='y',          \n",
    "        which='both',      \n",
    "        labelsize=20)      \n",
    "\n",
    "    label_dict = {0: \"Non\", 1: \"Lens\"}\n",
    "    labels_lab = [ label_dict[labels[0]], label_dict[labels[1]] ]  \n",
    "    plt.xticks(labels, labels_lab)\n",
    "    plt.yticks(labels, labels_lab)\n",
    "\n",
    "    make_dir(dir_analysis_figure)\n",
    "    file_save = dir_analysis_figure + \"confusion.png\"\n",
    "    plt.savefig(file_save)\n",
    "\n",
    "    return file_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True vs prediction plot for all test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tru_pred (y_train, y_pred_train, y_valid, y_pred_valid, y_test, y_pred):\n",
    "    fig, axis1 = plt.subplots(figsize=(8,8))\n",
    "    plt.scatter(y_test, y_pred, label='comparison')\n",
    "    plt.scatter(y_train, y_pred_train, s=3, label='train set')\n",
    "    plt.scatter(y_valid, y_pred_valid, s=5, label='valid set')\n",
    "    plt.plot([0,1], [0,1], 'k--', label=\"1-1\")\n",
    "    #plt.title(\"Receiver Operator Characteristic (ROC)\")\n",
    "    plt.xlabel(\"Truth\")\n",
    "    plt.ylabel(\"Prediction\")\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('regression_ytest'+cnn_label)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curve with AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(dir_analysis_figure, fpr, tpr):\n",
    "    \"\"\"Plot the ROC.\"\"\"\n",
    "    # plot\n",
    "\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_style(\"ticks\")\n",
    "\n",
    "    fig, axis1 = plt.subplots(figsize=(10,8))\n",
    "    sns.set_palette(\"coolwarm\", n_colors=6)\n",
    "    sns.set_color_codes()\n",
    "    lw = 3\n",
    "    legend_label_size = \"xx-large\"\n",
    "\n",
    "    plt.plot(fpr, tpr, 'r-', lw=lw, label=\"ROC curve\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw, label=\"1-1\")\n",
    "    plt.xlabel(\"False positive\", fontsize=25)\n",
    "    plt.ylabel(\"True positive\", fontsize=25)\n",
    "\n",
    "    axis1.grid(False)\n",
    "    axis1.tick_params(labeltop='off', labelright='off')\n",
    "    axis1.xaxis.set_ticks_position('bottom')\n",
    "    axis1.yaxis.set_ticks_position('left')\n",
    "\n",
    "    legend1 = plt.legend(loc='lower right', shadow=True, prop={'size':25})\n",
    "    for label in legend1.get_texts():\n",
    "            label.set_fontsize(legend_label_size)\n",
    "    for label in legend1.get_lines():\n",
    "        label.set_linewidth(lw)  # the legend line width\n",
    "\n",
    "    plt.tick_params(\n",
    "        axis='x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        top='off',\n",
    "        labelsize=20)         # ticks along the top edge are off\n",
    "    plt.tick_params(\n",
    "        axis='y',          \n",
    "        which='both',      \n",
    "        labelsize=20)      \n",
    "\n",
    "    # save to file\n",
    "    # make output directory if necessary\n",
    "    make_dir(dir_analysis_figure)\n",
    "    file_save = dir_analysis_figure + \"roc.png\"\n",
    "    plt.savefig(file_save)\n",
    "\n",
    "    # auc = metrics.roc_auc_score(ytrue, yscore)\n",
    "    # print \"auc:\", auc\n",
    "\n",
    "    return file_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of all output predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probability_distribution(dir_analysis_figure, y_prob_set):\n",
    "    \"\"\"Plot history as a function of epoch.\"\"\"\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_style(\"ticks\")\n",
    "\n",
    "    fig, axis1 = plt.subplots(figsize=(10,8))\n",
    "\n",
    "    n_colors = 4\n",
    "    sns.cubehelix_palette(n_colors, start=2.8, rot=.1) #, start=1, rot=1.2, as_cmap=True)\n",
    "    sns.set_palette(\"coolwarm\", n_colors=n_colors )\n",
    "    sns.set_color_codes()\n",
    "    lw = 5\n",
    "    legend_label_size = \"xx-large\"\n",
    "    nbins = 7\n",
    "\n",
    "\n",
    "    plt.title(\"Probability Distributions\")\n",
    "    plt.xlabel('probability', fontsize=20)\n",
    "        \n",
    "\n",
    "    axis1.grid(False)\n",
    "    axis1.tick_params(labeltop='off', labelright='off')\n",
    "    axis1.xaxis.set_ticks_position('bottom')\n",
    "    axis1.yaxis.set_ticks_position('left')\n",
    "\n",
    "\n",
    "    y_prob_all = y_prob_set['all']\n",
    "    y_prob_tp = y_prob_set['tp']\n",
    "    y_prob_fp = y_prob_set['fp']\n",
    "    y_prob_tn = y_prob_set['tn']\n",
    "    y_prob_fn = y_prob_set['fn']\n",
    "\n",
    "    hist_all, bins_all = np.histogram(y_prob_all, bins=nbins)\n",
    "    center_all = (bins_all[:-1] + bins_all[1:]) / 2\n",
    "\n",
    "    hist_tp, bins_tp = np.histogram(y_prob_tp, bins=nbins)\n",
    "    center_tp = (bins_tp[:-1] + bins_tp[1:]) / 2\n",
    "\n",
    "    hist_fp, bins_fp = np.histogram(y_prob_fp, bins=nbins)\n",
    "    center_fp = (bins_fp[:-1] + bins_fp[1:]) / 2\n",
    "\n",
    "    hist_tn, bins_tn = np.histogram(y_prob_tn, bins=nbins)\n",
    "    center_tn = (bins_tn[:-1] + bins_tn[1:]) / 2\n",
    "\n",
    "    hist_fn, bins_fn = np.histogram(y_prob_fn, bins=nbins)\n",
    "    center_fn = (bins_fn[:-1] + bins_fn[1:]) / 2\n",
    "\n",
    "    plt.yscale('log')\n",
    "    #plt.xscale('log')\n",
    "    axis1.plot(center_all, hist_all,  lw=lw, color='k', ls=\":\", label=\"All\")\n",
    "    axis1.plot(center_tp, hist_tp,  lw=lw, ls='-', label=\"TP\")\n",
    "    axis1.plot(center_fp, hist_fp,  lw=lw, ls='-', label=\"FP\")\n",
    "    axis1.plot(center_tn, hist_tn, lw=lw, ls='--', label=\"TN\")\n",
    "    axis1.plot(center_fn, hist_fn,  lw=lw, ls='--', label=\"FN\")\n",
    "\n",
    "    legend1 = plt.legend(loc='upper left', shadow=True, prop={'size':25})\n",
    "    for label in legend1.get_texts():\n",
    "        label.set_fontsize(legend_label_size)\n",
    "    for label in legend1.get_lines():\n",
    "        label.set_linewidth(lw)  # the legend line width\n",
    "\n",
    "    plt.tick_params(\n",
    "        axis='x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        top='off',\n",
    "        labelsize=20)         # ticks along the top edge are off\n",
    "    plt.tick_params(\n",
    "        axis='y',          \n",
    "        which='both',      \n",
    "        labelsize=20)       \n",
    "\n",
    "    make_dir(dir_analysis_figure)\n",
    "    file_save = dir_analysis_figure + \"probability_distribution.png\"\n",
    "    plt.savefig(file_save)\n",
    "\n",
    "    return file_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision / recall / f1 / brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not a plot but we need to print these out\n",
    "def scores(y_test, probability, prediction):\n",
    "# ------------------------------------------------------------------------------\n",
    "# Evaluate classificaiton\n",
    "# Outputs the accuracy, precision, recall, f1 score, brier score of classificaiton\n",
    "# ------------------------------------------------------------------------------\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_test, prediction)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, prediction)\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, prediction)\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_test, prediction)\n",
    "    print('F1 score: %f' % f1)\n",
    "    #brier score\n",
    "    br = brier_score_loss(y_test, probability)\n",
    "    print('Brier score is: %f' % br)\n",
    "    return accuracy, precision, recall, f1, br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How these change if the probability treshold fro class changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_F1(df, mass_thresh='2e14', xlim=None, method='cnn'):\n",
    "    if xlim is None:\n",
    "    xlim = (0.8, 0.999) if method == 'cnn' else (3., 30.)\n",
    "    col = f'score_wdust (trained>{mass_thresh})' if method == 'cnn' else 'mf_peaksig'\n",
    "    Fscore = lambda x: _get_Fbeta(df[f'Truth(>{mass_thresh})'], (df[col]>x).astype(int))\n",
    "    f = plt.figure(figsize=(12, 4))\n",
    "    x = np.linspace(xlim[0], xlim[1])\n",
    "    y = [Fscore(xx) for xx in x]\n",
    "    plt.scatter(x, y)\n",
    "    plt.xlim(xlim)\n",
    "    plt.xlabel('%s thres'%(\"CNN prob\" if method == 'cnn' else \"MF S/N\"), fontsize='xx-large')\n",
    "    plt.ylabel('F1_score', fontsize='xx-large')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoD_histogram (prob_all, prob_TP_TN, variable_all, variable_TP_TN):\n",
    "    # Plot 2D histogram of the distribution of all future mergers vs TP future mergers \n",
    "    #(stellar mass and the output probability)\n",
    "    sns.set_style(\"white\")\n",
    "    plt.ylabel('CNN Output')\n",
    "    plt.xlabel('Stellar Mass')\n",
    "    plt.xlim(9.4, 11.8)\n",
    "    plt.xticks([9.5, 10, 10.5, 11, 11.5])\n",
    "    sns.kdeplot(variable_TP_TN, prob_TP_TN, cmap=\"RdGy\",  n_levels=10)\n",
    "    sns.kdeplot(variable_all_P_N, prob_all_P_N, cmap=\"coolwarm\", n_levels=10)\n",
    "\n",
    "    r = sns.color_palette(\"RdGy\")[0]\n",
    "    b = sns.color_palette(\"coolwarm\")[0]\n",
    "\n",
    "    red_patch = mpatches.Patch(color=r, label='TP/TN')\n",
    "    blue_patch = mpatches.Patch(color=b, label='all positives/negatives')\n",
    "    plt.legend(handles=[red_patch,blue_patch],loc='lower right')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstraping and errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bootstraped errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstraping_scores (y_test, probability, prediction, boot_num, rand_seed):\n",
    "    y_pred = probability\n",
    "    y_true = y_test\n",
    "    pred = prediction\n",
    "\n",
    "    #print(\"Original ROC area: {:0.3f}\".format(roc_auc_score(y_true, y_pred)))\n",
    "\n",
    "    n_bootstraps = boot_num#1000\n",
    "    rng_seed = rand_seed#3  # control reproducibility\n",
    "    bootstrapped_roc_auc = []\n",
    "    bootstrapped_accuracy = []\n",
    "    bootstrapped_precision = []\n",
    "    bootstrapped_recall = []\n",
    "    bootstrapped_f1 = []\n",
    "    bootstrapped_brier = []\n",
    "\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    for i in range(n_bootstraps):\n",
    "        # bootstrap by sampling with replacement on the prediction indices\n",
    "        indices = rng.randint(0, len(y_pred) - 1, len(y_pred))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            # We need at least one positive and one negative sample for ROC AUC\n",
    "            # to be defined: reject the sample\n",
    "            continue\n",
    "\n",
    "        roc_auc = roc_auc_score(y_true[indices], y_pred[indices])\n",
    "        bootstrapped_roc_auc.append(roc_auc)\n",
    "        #print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "   \n",
    "        score_acc = accuracy_score(y_true[indices], pred[indices])\n",
    "        bootstrapped_accuracy.append(score_acc)\n",
    "    \n",
    "        score_precision = precision_score(y_true[indices], pred[indices])\n",
    "        bootstrapped_precision.append(score_precision)\n",
    "    \n",
    "        score_recall = recall_score(y_true[indices], pred[indices])\n",
    "        bootstrapped_recall.append(score_recall)\n",
    "    \n",
    "        score_f1 = f1_score(y_true[indices], pred[indices])\n",
    "        bootstrapped_f1.append(score_f1)\n",
    "    \n",
    "        score_brier = brier_score_loss(y_true[indices], y_pred[indices])\n",
    "        bootstrapped_brier.append(score_brier)\n",
    "        \n",
    "        \n",
    "    return bootstrapped_roc_auc,bootstrapped_accuracy, bootstrapped_precision,bootstrapped_recall,bootstrapped_f1,bootstrapped_brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval (bootstrapped_score,score):\n",
    "    sorted_scores = np.array(bootstrapped_score)\n",
    "    sorted_scores.sort()\n",
    "\n",
    "    # Computing the lower and upper bound of the 90% confidence interval\n",
    "    # You can change the bounds percentiles to 0.025 and 0.975 to get\n",
    "    # a 95% confidence interval instead.\n",
    "    confidence_lower = sorted_scores[int(0.05 * len(sorted_scores))]\n",
    "    confidence_upper = sorted_scores[int(0.95 * len(sorted_scores))]\n",
    "\n",
    "    confidence_lower1 = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "    confidence_upper1 = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "\n",
    "    print(\"Original ROC area: {:0.3f}\".format(score))\n",
    "\n",
    "    print(\"90% Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "        confidence_lower, confidence_upper))\n",
    "    print(\"95% Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "        confidence_lower1, confidence_upper1))\n",
    "    print(\"95% Errors are: [{:0.3f} , {:0.3}]\".format(\n",
    "        confidence_lower1-score, confidence_upper1-score),\"\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bootstraped histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstraped_hist (bootstrapped_roc_auc, bootstrapped_accuracy, bootstrapped_precision, bootstrapped_recall, bootstrapped_f1, bootstrapped_brier):\n",
    "    plt.hist(bootstrapped_roc_auc, bins=50)\n",
    "    plt.title('Histogram of the bootstrapped ROC AUC scores')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(bootstrapped_accuracy, bins=50)\n",
    "    plt.title('Histogram of the bootstrapped accuraciy scores')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(bootstrapped_precision, bins=50)\n",
    "    plt.title('Histogram of the bootstrapped precision scores')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(bootstrapped_recall, bins=50)\n",
    "    plt.title('Histogram of the bootstrapped recall scores')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(bootstrapped_f1, bins=50)\n",
    "    plt.title('Histogram of the bootstrapped F1 scores')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bootstraped ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_with_CI (y_test, prob, num_boot, rand_seed):\n",
    "\n",
    "    y_pred = prob\n",
    "    y_true = y_test\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, prob, pos_label=1)\n",
    "    \n",
    "    n_bootstraps = num_boot # 1000\n",
    "    rng_seed = rand_seed #3  # control reproducibility\n",
    "    bootstrapped_fpr = []\n",
    "    bootstrapped_tpr = []\n",
    "\n",
    "\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    for i in range(n_bootstraps):\n",
    "        # bootstrap by sampling with replacement on the prediction indices\n",
    "        indices = rng.randint(0, len(y_pred) - 1, len(y_pred))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            # We need at least one positive and one negative sample for ROC AUC\n",
    "            # to be defined: reject the sample\n",
    "            continue\n",
    "        fpr_score, tpr_score, thresholds_score = metrics.roc_curve(y_true[indices], y_pred[indices], pos_label=1)\n",
    "\n",
    "        bootstrapped_fpr.append(fpr_score)\n",
    "        bootstrapped_tpr.append(tpr_score)\n",
    "\n",
    "        \n",
    "    tprs = []\n",
    "    base_fpr = np.linspace(0, 1, 1001)   \n",
    "    for i in range(len(bootstrapped_fpr)):    \n",
    "        tpr1 = interp(base_fpr, bootstrapped_fpr[i], bootstrapped_tpr[i])\n",
    "        tpr1[0] = 0.0\n",
    "        tprs.append(tpr1)\n",
    "\n",
    "    tprs = np.array(tprs)\n",
    "    mean_tprs = tprs.mean(axis=0)\n",
    "    std = tprs.std(axis=0)\n",
    "\n",
    "    tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "    tprs_lower = mean_tprs - std\n",
    "\n",
    "    #https://dfrieds.com/math/confidence-intervals   for 95%CI\n",
    "    tprs_upper_95 = mean_tprs - 1.96*std \n",
    "    tprs_lower_95 = mean_tprs + 1.96*std\n",
    "    \n",
    "        \n",
    "    #plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    x_onetoone = y_onetoone = [0, 1]\n",
    "    prist = plt.plot(fpr, tpr, 'navy', linewidth=2, label='pristine images')\n",
    "    prist1 = plt.fill_between(base_fpr, tprs_lower_95, tprs_upper_95, color='deepskyblue', alpha=0.9, label='95%CI pristine images')\n",
    "    line11 = plt.plot(x_onetoone, y_onetoone, 'k--',linewidth=1, label=\"1-1\")\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.xlabel(\"False Positive (1 - Specificity)\")\n",
    "    plt.ylabel(\"True Positive (Selectivity)\")\n",
    "    plt.tight_layout()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(params, params_model, dirs, figure_dict, data_analysis):\n",
    "    \"\"\"Create report.\"\"\"\n",
    "    # report filename\n",
    "    filename_report = dirs['analysis'] + \"report_\" + str(params[\"id_model\"]).zfill(3) + \"_\" + str(params[\"id_analysis\"]).zfill(3)\n",
    "    \n",
    "\n",
    "    # Create Document\n",
    "    # geometry_options = {\"tmargin\": \"-2.875in\", \"oddsidemargin\": \"-.875in\", \"evensidemargin\":\"-.875in\", \n",
    "    #                    \"textwidth\":\"0.75in\", \"textheight\": \"2.75in\"}\n",
    "    #geometry_options = {\"tmargin\": \"-.875in\", \"lmargin\": \"1.5cm\", \"footskip\":\"30pt\", \"textheight\":\"\"}\n",
    "    #geometry_options = {\"margin\": \"-0.5in\"}\n",
    "\n",
    "    # create document\n",
    "    doc = Document() #geometry_options=geometry_options)\n",
    "\n",
    "    doc.preamble.append(NoEscape(r'\\usepackage{float}'))\n",
    "    #doc.preamble.append(NoEscape(r'\\extrafloats{100}'))\n",
    "\n",
    "    # Display basic analysis parameters\n",
    "    with doc.create(Section('Analysis Parameters')):\n",
    "        with doc.create(Tabular('c|c|c')) as table:\n",
    "            table.add_row((\"name\", \"id number\", 'number of objects'))\n",
    "            table.add_hline()\n",
    "            table.add_hline()\n",
    "            table.add_row((\"id_model\", params[\"id_model\"], '-'))\n",
    "            table.add_hline()\n",
    "            table.add_row((\"id_analysis\", params[\"id_analysis\"], '-'))\n",
    "            table.add_hline()\n",
    "            table.add_row((\"id_data_train\", params[\"id_data_train\"], params_model[\"nb_train\"]))\n",
    "            table.add_hline()\n",
    "            table.add_row((\"id_data_valid\", params[\"id_data_valid\"], params_model[\"nb_valid\"]))\n",
    "            table.add_hline()\n",
    "            table.add_row((\"id_data_test\", params[\"id_data_test\"], params[\"nb_test\"]))\n",
    "            table.add_hline()\n",
    "\n",
    "\n",
    "    with doc.create(Section('Confusion Diagnostics')):\n",
    "        with doc.create(Tabular('c|c')) as table:\n",
    "            table.add_hline()\n",
    "            table.add_hline()\n",
    "            table.add_row(('AUC', data_analysis['roc']['auc']))\n",
    "            table.add_hline()\n",
    "            table.add_hline()\n",
    "            table.add_row(('Quality', data_analysis['quality']))\n",
    "            table.add_hline()\n",
    "            table.add_hline()\n",
    "            table.add_row(('Confusion Elements', ''))\n",
    "            table.add_hline()\n",
    "            table.add_hline()\n",
    "            for key, value in data_analysis['cm_elem'].items():\n",
    "                table.add_row((key,value))\n",
    "                table.add_hline()\n",
    "\n",
    "    # model parameters\n",
    "    with doc.create(Section('Training Parameters')):\n",
    "        with doc.create(Tabular('c|c')) as table:\n",
    "            table.add_hline()\n",
    "            table.add_hline()\n",
    "            metadata = dm.load_metadata_log(dirs['learnlist'], id_model=params[\"id_model\"])\n",
    "            #print 'metadata type', isinstance(metadata, dict)\n",
    "            for key, val in metadata.items():\n",
    "                table.add_row((key,val))\n",
    "                table.add_hline()\n",
    "\n",
    "    doc.append(NewPage()) \n",
    "\n",
    "    # model parameters\n",
    "    with doc.create(Section('Model Summary')):\n",
    "        file_model_layers = dirs[\"model\"] + \"model_summary.txt\"\n",
    "        f = open(file_model_layers, 'r')\n",
    "        x = f.read()\n",
    "        f.close()\n",
    "        doc.append(x)\n",
    "\n",
    "    doc.append(NewPage()) \n",
    "    # Diagnostics Figures\n",
    "    with doc.create(Section('Diagnostic Figures')):\n",
    "        doc.append(NewPage()) \n",
    "        ifig = 0\n",
    "        print \"figures\"\n",
    "        for key, val in figure_dict.items():\n",
    "            try:\n",
    "                print ifig, key, val\n",
    "                with doc.create(Figure(position='')) as temp_object:\n",
    "                    temp_object.add_image(figure_dict[key]['filename'], width=figure_dict[key]['width'])\n",
    "                    temp_object.add_caption(key)\n",
    "                ifig+=1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if ifig % 10 == 0:\n",
    "                 doc.append(NoEscape(r'\\clearpage'))\n",
    "            #    doc.append(NewPage())\n",
    "   \n",
    "\n",
    "\n",
    "    # generate pdf\n",
    "    doc.generate_pdf(filename_report, clean_tex=False)\n",
    "    return filename_report + \".pdf\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
