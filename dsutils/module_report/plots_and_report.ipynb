{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system level\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "# arrays\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from scipy import interp\n",
    "\n",
    "# keras\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras import models\n",
    "\n",
    "# sklearn (for machine learning)\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from scipy.stats import sem\n",
    "\n",
    "#model plotting\n",
    "import pydotplus\n",
    "import keras.utils\n",
    "\n",
    "# plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import pylab as pl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import LogNorm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "from vis.visualization import visualize_cam\n",
    "from vis.utils import utils\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# reporting\n",
    "from pylatex import Document, Section, Subsection, Tabular, Math, TikZ, Axis, FlushLeft, MediumText\n",
    "from pylatex import Plot, Figure, Matrix, Alignat, MultiColumn, Command, SubFigure, NoEscape, HorizontalSpace\n",
    "from pylatex.utils import italic, bold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'SB00_history_68_4_14.json'\n",
    "if os.path.exists(path): # reload history if it exists\n",
    "        with codecs.open(path, 'r', encoding='utf-8') as f:\n",
    "             n = json.loads(f.read())\n",
    "\n",
    "#Load Model\n",
    "\n",
    "json_file = open('SB00_model_68_4_14.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load best weights into new model\n",
    "loaded_model.load_weights('SB00_weights_68_4_14.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = np.load('prob.npy')\n",
    "pred = np.load('pred.npy')\n",
    "fpr = np.load('fpr.npy')\n",
    "tpr = np.load('tpr.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plottng Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite to plot all 4 images TP,TN/FP,FN\n",
    "\n",
    "def examples_plot(images, nrows, ncols):#WORKS\n",
    "# ------------------------------------------------------------------------------\n",
    "# Funciton plots images given in examples\n",
    "# ------------------------------------------------------------------------------\n",
    "    fig1=plt.figure(figsize=(5,5))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(nrows, ncols, i + 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(image, aspect='auto', cmap='viridis', norm=LogNorm())\n",
    "    plt.subplots_adjust(hspace=0, wspace=0)\n",
    "    plt.show()\n",
    "    plt.savefig('images/examples1.pdf')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_plot_model(json_file):#WORKS\n",
    "# ------------------------------------------------------------------------------\n",
    "# Funciton loads model architecture and plotsit\n",
    "# ------------------------------------------------------------------------------\n",
    "    json_file = open(json_file, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    keras.utils.vis_utils.pydot = pydotplus\n",
    "    keras.utils.plot_model(loaded_model, to_file='images/model.pdf', show_shapes=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True labels vs predicted output probabilities for train, validation and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_pred (y_test, y_prob):#WORKS!\n",
    "# ------------------------------------------------------------------------------\n",
    "# Function plots true labels vs predictions for train, validaiton and test set\n",
    "# ------------------------------------------------------------------------------\n",
    "    fig, axis1 = plt.subplots(figsize=(8,8))\n",
    "    plt.scatter(y_test, y_pred, label='test')\n",
    "    plt.plot([0,1], [0,1], 'k--', label=\"1-1\")\n",
    "    plt.xlabel(\"Truth\")\n",
    "    plt.ylabel(\"Prediction\")\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/true_pred.pdf')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss/Accuracy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_acc_plot(loss, val_loss, acc, val_acc, epochs):#WORKS!\n",
    "# ------------------------------------------------------------------------------\n",
    "# Funciton plots a combined loss and accuracy plot for training and validation set\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "    figsize=(6,4)\n",
    "    fig, axis1 = plt.subplots(figsize=figsize)\n",
    "    plot1_lacc = axis1.plot(epochs, acc, 'navy', label='accuracy')\n",
    "    plot1_val_lacc = axis1.plot(epochs, val_acc, 'deepskyblue', label=\"validation accuracy\")\n",
    "\n",
    "    plot1_loss = axis1.plot(epochs, loss, 'red', label='loss')\n",
    "    plot1_val_loss = axis1.plot(epochs, val_loss, 'lightsalmon', label=\"validation loss\")\n",
    "\n",
    "\n",
    "    plots = plot1_loss + plot1_val_loss\n",
    "    labs = [l.get_label() for l in plots]\n",
    "    axis1.set_xlabel('Epoch')\n",
    "    axis1.set_ylabel('Loss/Accuracy')\n",
    "    plt.tight_layout()\n",
    "    axis1.legend(loc='center right')\n",
    "    plt.savefig('images/loss_acc.pdf')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision/Recall Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec_recall_plot(y_test, probability):#WORKS!\n",
    "# ------------------------------------------------------------------------------\n",
    "# Funciton plots a combined precision and recall plot for training and validation set\n",
    "# ------------------------------------------------------------------------------\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, probability)\n",
    "\n",
    "    figsize=(6,4)\n",
    "    plt.plot(precision, recall, 'r')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/prec_recall.pdf')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix plot with original and normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CM_ROC(x_test,y_test):#WORKS\n",
    "# ------------------------------------------------------------------------------\n",
    "# Evaluate classificaiton\n",
    "# Outputs the confusion matrix and ROC curve vith its AUC \n",
    "# ------------------------------------------------------------------------------\n",
    "    # predict\n",
    "    prob = loaded_model.predict(x_test)\n",
    "    pred =  (prob > 0.5).astype('int32') \n",
    "\n",
    "    # measure confusion\n",
    "    labels=[0, 1]\n",
    "    cm = metrics.confusion_matrix(y_test, pred, labels=labels)\n",
    "    cm = cm.astype('float')\n",
    "    cm_norm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"cm\", cm)\n",
    "    print(\"cm_norm\", cm_norm)\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, prob, pos_label=1)\n",
    "    auc = metrics.roc_auc_score(y_test, prob)\n",
    "    np.save('images/auc.npy',auc)\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "    #plotting\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion matrix (Pristine Images)',y=1.08)\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i in range(cm_norm.shape[0]):\n",
    "        for j in range(cm_norm.shape[1]):\n",
    "            ax.text(j, i, format(cm_norm[i, j], fmt),\n",
    "            ha=\"center\", va=\"center\",\n",
    "            color=\"white\" if cm_norm[i, j] < thresh else \"black\")\n",
    "    pl.savefig('images/conf.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    #ROC\n",
    "    figsize=(5,5)\n",
    "    fig, axis1 = plt.subplots(figsize=figsize)\n",
    "    x_onetoone = y_onetoone = [0, 1]\n",
    "\n",
    "    plt.plot(fpr, tpr, 'r-')\n",
    "    plt.plot(x_onetoone, y_onetoone, 'k--',  label=\"1-1\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.title(\"Receiver Operator Characteristic (ROC)\")\n",
    "    plt.xlabel(\"False Positive (1 - Specificity)\")\n",
    "    plt.ylabel(\"True Positive (Selectivity)\")\n",
    "    plt.tight_layout()\n",
    "    pl.savefig('images/ROC.pdf')\n",
    "    return prob, pred, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision / recall / f1 / brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(y_test, probability, prediction):#WORKS\n",
    "# ------------------------------------------------------------------------------\n",
    "# Evaluate classificaiton\n",
    "# Outputs the accuracy, precision, recall, f1 score, brier score of classificaiton\n",
    "# ------------------------------------------------------------------------------\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_test, prediction)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, prediction)\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, prediction)\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_test, prediction)\n",
    "    print('F1 score: %f' % f1)\n",
    "    #brier score\n",
    "    br = brier_score_loss(y_test, probability)\n",
    "    print('Brier score is: %f' % br)\n",
    "    scoring = np.array([accuracy, precision, recall, f1, br])\n",
    "    np.save('images/scoring.npy',scoring)\n",
    "    return accuracy, precision, recall, f1, br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of output probailities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(outputs, num_class, bin_num):#WORKS\n",
    "# ------------------------------------------------------------------------------\n",
    "# Funciton plots a nice histogram for 3 merger subsamples\n",
    "# ------------------------------------------------------------------------------\n",
    "    bins = bin_num\n",
    "    for i in range(num_class):\n",
    "        plt.hist(outputs[i], bins, alpha=0.9, label='class_'+str(i+1))\n",
    "        plt.xlabel(\"CNN Output\")\n",
    "        plt.ylabel(\"Frequency in test set\")\n",
    "        plt.savefig('images/histogram.pdf')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoD_histogram (prob_all, prob_TP_TN, variable_all, variable_TP_TN):#WORKS\n",
    "# ------------------------------------------------------------------------------\n",
    "# Plot 2D histogram of the distribution of all entire positive/negative class vs TP/TN\n",
    "# vs one object parameter\n",
    "# ------------------------------------------------------------------------------\n",
    "    sns.set_style(\"white\")\n",
    "    plt.ylabel('CNN Output')\n",
    "    plt.xlabel('Stellar Mass')\n",
    "    plt.xlim(9.4, 11.8)\n",
    "    plt.xticks([9.5, 10, 10.5, 11, 11.5])\n",
    "    sns.kdeplot(variable_TP_TN, prob_TP_TN, cmap=\"RdGy\",  n_levels=10)\n",
    "    sns.kdeplot(variable_all_P_N, prob_all_P_N, cmap=\"coolwarm\", n_levels=10)\n",
    "\n",
    "    r = sns.color_palette(\"RdGy\")[0]\n",
    "    b = sns.color_palette(\"coolwarm\")[0]\n",
    "\n",
    "    red_patch = mpatches.Patch(color=r, label='TP/TN')\n",
    "    blue_patch = mpatches.Patch(color=b, label='all positives/negatives')\n",
    "    plt.legend(handles=[red_patch,blue_patch],loc='lower right')\n",
    "    plt.show()\n",
    "    plt.savefig('images/2Dhistogram.pdf')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstraping and errors, extra plots that we might add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bootstraped errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstraping_scores (y_test, probability, prediction, boot_num, rand_seed):#WORKS\n",
    "# ------------------------------------------------------------------------------\n",
    "# Produces bootstraped errors for all scoring methods\n",
    "# auc, accuracy, precision, recall, f1 score, brier score\n",
    "# ------------------------------------------------------------------------------    \n",
    "\n",
    "    y_pred = probability\n",
    "    y_true = y_test\n",
    "    pred = prediction\n",
    "\n",
    "\n",
    "    n_bootstraps = boot_num     #number of bootstrap samples we want\n",
    "    rng_seed = rand_seed        # control reproducibility\n",
    "    bootstrapped_roc_auc = []\n",
    "    bootstrapped_accuracy = []\n",
    "    bootstrapped_precision = []\n",
    "    bootstrapped_recall = []\n",
    "    bootstrapped_f1 = []\n",
    "    bootstrapped_brier = []\n",
    "\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    for i in range(n_bootstraps):\n",
    "        # bootstrap by sampling with replacement on the prediction indices\n",
    "        indices = rng.randint(0, len(y_pred) - 1, len(y_pred))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            # We need at least one positive and one negative sample for ROC AUC\n",
    "            # to be defined: reject the sample\n",
    "            continue\n",
    "\n",
    "        roc_auc = roc_auc_score(y_true[indices], y_pred[indices])\n",
    "        bootstrapped_roc_auc.append(roc_auc)\n",
    "   \n",
    "        score_acc = accuracy_score(y_true[indices], pred[indices])\n",
    "        bootstrapped_accuracy.append(score_acc)\n",
    "    \n",
    "        score_precision = precision_score(y_true[indices], pred[indices])\n",
    "        bootstrapped_precision.append(score_precision)\n",
    "    \n",
    "        score_recall = recall_score(y_true[indices], pred[indices])\n",
    "        bootstrapped_recall.append(score_recall)\n",
    "    \n",
    "        score_f1 = f1_score(y_true[indices], pred[indices])\n",
    "        bootstrapped_f1.append(score_f1)\n",
    "    \n",
    "        score_brier = brier_score_loss(y_true[indices], y_pred[indices])\n",
    "        bootstrapped_brier.append(score_brier)\n",
    "        \n",
    "        \n",
    "    return bootstrapped_roc_auc,bootstrapped_accuracy, bootstrapped_precision,bootstrapped_recall,bootstrapped_f1,bootstrapped_brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval (bootstrapped_score,score):#WORKS\n",
    "# ------------------------------------------------------------------------------\n",
    "# Calculates 95% CI from the bootstraped values\n",
    "# ------------------------------------------------------------------------------\n",
    "    sorted_scores = np.array(bootstrapped_score)\n",
    "    sorted_scores.sort()\n",
    "\n",
    "    # Computing the lower and upper bound of the 90% confidence interval\n",
    "    # You can change the bounds percentiles to 0.025 and 0.975 to get\n",
    "    # a 95% confidence interval instead.\n",
    "    confidence_lower = sorted_scores[int(0.05 * len(sorted_scores))]\n",
    "    confidence_upper = sorted_scores[int(0.95 * len(sorted_scores))]\n",
    "\n",
    "    confidence_lower1 = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "    confidence_upper1 = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "\n",
    "    print(\"Original ROC area: {:0.3f}\".format(score))\n",
    "\n",
    "    print(\"90% Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "        confidence_lower, confidence_upper))\n",
    "    print(\"95% Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "        confidence_lower1, confidence_upper1))\n",
    "    print(\"95% Errors are: [{:0.3f} , {:0.3}]\".format(\n",
    "        confidence_lower1-score, confidence_upper1-score),\"\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bootstraped histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKS\n",
    "def bootstraped_hist (bootstrapped_roc_auc, bootstrapped_accuracy, bootstrapped_precision, bootstrapped_recall, bootstrapped_f1, bootstrapped_brier):\n",
    "# ------------------------------------------------------------------------------\n",
    "# Plot all bootstrap histograms\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "    plt.hist(bootstrapped_roc_auc, bins=50)\n",
    "    plt.title('Histogram of the bootstrapped ROC AUC scores')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(bootstrapped_accuracy, bins=50)\n",
    "    plt.title('Histogram of the bootstrapped accuraciy scores')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(bootstrapped_precision, bins=50)\n",
    "    plt.title('Histogram of the bootstrapped precision scores')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(bootstrapped_recall, bins=50)\n",
    "    plt.title('Histogram of the bootstrapped recall scores')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(bootstrapped_f1, bins=50)\n",
    "    plt.title('Histogram of the bootstrapped F1 scores')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bootstraped ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_with_CI (y_test, prob, num_boot, rand_seed):#WORKS\n",
    "# ------------------------------------------------------------------------------\n",
    "# Bootstrap the ROC curve in y directon\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "    y_pred = prob\n",
    "    y_true = y_test\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, prob, pos_label=1)\n",
    "    \n",
    "    n_bootstraps = num_boot   #number of bootstrap samples we want\n",
    "    rng_seed = rand_seed      # control reproducibility\n",
    "    bootstrapped_fpr = []\n",
    "    bootstrapped_tpr = []\n",
    "\n",
    "\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    for i in range(n_bootstraps):\n",
    "        # bootstrap by sampling with replacement on the prediction indices\n",
    "        indices = rng.randint(0, len(y_pred) - 1, len(y_pred))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            # We need at least one positive and one negative sample for ROC AUC\n",
    "            # to be defined: reject the sample\n",
    "            continue\n",
    "        fpr_score, tpr_score, thresholds_score = metrics.roc_curve(y_true[indices], y_pred[indices], pos_label=1)\n",
    "\n",
    "        bootstrapped_fpr.append(fpr_score)\n",
    "        bootstrapped_tpr.append(tpr_score)\n",
    "\n",
    "        \n",
    "    tprs = []\n",
    "    base_fpr = np.linspace(0, 1, 1001)   \n",
    "    for i in range(len(bootstrapped_fpr)):    \n",
    "        tpr1 = interp(base_fpr, bootstrapped_fpr[i], bootstrapped_tpr[i])\n",
    "        tpr1[0] = 0.0\n",
    "        tprs.append(tpr1)\n",
    "\n",
    "    tprs = np.array(tprs)\n",
    "    mean_tprs = tprs.mean(axis=0)\n",
    "    std = tprs.std(axis=0)\n",
    "\n",
    "    tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "    tprs_lower = mean_tprs - std\n",
    "\n",
    "    #https://dfrieds.com/math/confidence-intervals   for 95%CI\n",
    "    tprs_upper_95 = mean_tprs - 1.96*std \n",
    "    tprs_lower_95 = mean_tprs + 1.96*std\n",
    "    \n",
    "        \n",
    "    #plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    x_onetoone = y_onetoone = [0, 1]\n",
    "    prist = plt.plot(fpr, tpr, 'navy', linewidth=2, label='pristine images')\n",
    "    prist1 = plt.fill_between(base_fpr, tprs_lower_95, tprs_upper_95, color='deepskyblue', alpha=0.9, label='95%CI pristine images')\n",
    "    line11 = plt.plot(x_onetoone, y_onetoone, 'k--',linewidth=1, label=\"1-1\")\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.xlabel(\"False Positive (1 - Specificity)\")\n",
    "    plt.ylabel(\"True Positive (Selectivity)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/ROC_CI.pdf')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report():\n",
    "#if __name__ == '__main__':\n",
    "    #image_filename = os.path.join(os.path.dirname(__file__), 'histogram.jpg')\n",
    "    example_TP = 'images/examples1.pdf'\n",
    "    example_FP = 'images/examples2.pdf'\n",
    "    example_TN = 'images/examples1.pdf'\n",
    "    example_FN = 'images/examples2.pdf'\n",
    "    \n",
    "    model = 'images/model.pdf'\n",
    "    \n",
    "    lo_acc = 'images/loss_acc.pdf'\n",
    "    pr_rec = 'images/prec_recall.pdf'\n",
    "    \n",
    "    hist = 'images/histogram.pdf'\n",
    "    hist_2d = 'images/2Dhistogram.pdf'\n",
    "    \n",
    "    conf_matrix = 'images/conf.pdf'\n",
    "    roc = 'images/ROC.pdf'\n",
    "    #roc_ci = 'images/ROC_CI.pdf'\n",
    "    \n",
    "    true_pred = 'images/true_pred.pdf'\n",
    "    \n",
    "    scoring = np.load('images/scoring.npy')\n",
    "    auc_save = np.load('images/auc.npy')\n",
    "    auc = auc_save\n",
    "    acc = scoring[0]\n",
    "    precision = scoring[1]\n",
    "    recall = scoring[2]\n",
    "    f1 = scoring[3]\n",
    "    brier = scoring[4]\n",
    "    \n",
    "    \n",
    "    geometry_options = {\"tmargin\": \"1.5cm\", \"lmargin\": \"2.5cm\"}\n",
    "    doc = Document(geometry_options=geometry_options)\n",
    "\n",
    "    with doc.create(Section('CLASSIFICATION REPORT', numbering=0)):\n",
    "        \n",
    "        \n",
    "        # CNN architecture\n",
    "        with doc.create(Subsection('Architecture of the Neural Network', numbering=0)):\n",
    "            with doc.create(Figure(position='h!')) as loss_acc:\n",
    "                loss_acc.add_image(model, width='200px')   \n",
    "                \n",
    "        \n",
    "        # plot some example images\n",
    "        with doc.create(Subsection('TP/FP/TN/FN Test Set Examples', numbering=0)):\n",
    "            doc.append('TP - true positives, TN - true negatives, FP - false positives, FN - False negaties')\n",
    "            with doc.create(Figure(position='h!')) as imagesRow1:\n",
    "                doc.append(Command('centering'))  \n",
    "                with doc.create(\n",
    "                    SubFigure(position='c',  width=NoEscape(r'0.33\\linewidth'))) as left_image:\n",
    "                    left_image.add_image(example_TP, width=NoEscape(r'0.95\\linewidth'))\n",
    "                    left_image.add_caption(\"Examples of TP\")\n",
    "                \n",
    "                with doc.create(\n",
    "                    SubFigure(position='c', width=NoEscape(r'0.33\\linewidth'))) as right_image:\n",
    "                    right_image.add_image(example_FP, width=NoEscape(r'0.95\\linewidth'))\n",
    "                    right_image.add_caption(\"Examples of FP\")\n",
    "                \n",
    "            with doc.create(Figure(position='h!')) as imagesRow2:\n",
    "                doc.append(Command('centering'))  \n",
    "                with doc.create(\n",
    "                    SubFigure(position='c',  width=NoEscape(r'0.33\\linewidth'))) as left_image:\n",
    "                    left_image.add_image(example_TN, width=NoEscape(r'0.95\\linewidth'))\n",
    "                    left_image.add_caption(\"Examples of TN\")\n",
    "                \n",
    "                with doc.create(\n",
    "                    SubFigure(position='c', width=NoEscape(r'0.33\\linewidth'))) as right_image:\n",
    "                    right_image.add_image(example_FN, width=NoEscape(r'0.95\\linewidth'))\n",
    "                    right_image.add_caption(\"Examples of FN\")\n",
    "                \n",
    "                \n",
    "         # True values VS predicted output values\n",
    "        with doc.create(Subsection('Comparison of True Labes and Output Values for Test Set:', numbering=0)):\n",
    "            with doc.create(Figure(position='h!')) as tr_pr:\n",
    "                tr_pr.add_image(true_pred, width='200px') \n",
    "                \n",
    "        # Training loss / accuracy\n",
    "        with doc.create(Subsection('Training and Validation Loss and Accuracy:', numbering=0)):\n",
    "            with doc.create(Figure(position='h!')) as loss_acc:\n",
    "                loss_acc.add_image(lo_acc, width='260px')        \n",
    "                \n",
    "        # Training precision / recall\n",
    "        with doc.create(Subsection('Test Set Precission and Recall:', numbering=0)):\n",
    "            with doc.create(Figure(position='h!')) as pre_recall:\n",
    "                pre_recall.add_image(pr_rec, width='260px')\n",
    "                \n",
    "                \n",
    "        # plot confusion matrix\n",
    "        with doc.create(Subsection('Test Set Confusion Matrix', numbering=0)):\n",
    "            with doc.create(Figure(position='h!')) as conf:\n",
    "                conf.add_image(conf_matrix, width='210px')\n",
    "                \n",
    "                \n",
    "        # all scorin matrics\n",
    "        with doc.create(Subsection('Classification Scoring for Test Set', numbering=0)):\n",
    "            doc.append('TP - true positives, TN - true negatives, FP - false positives, FN - False negaties \\n\\n')\n",
    "            doc.append('The performance of a classifier can be described by:\\n')\n",
    "            doc.append(bold('Accuracy '))\n",
    "            doc.append(' - (TP+TN)/(TP+TN+FP+FN) \\n')\n",
    "            doc.append(bold('Precision '))\n",
    "            doc.append(' (Purity, Positive Predictive Value) - TP/(TP+FP) \\n')\n",
    "            doc.append(bold('Recall '))\n",
    "            doc.append(' (Completeness, True Positive Rate - TP/(TP+FN) \\n ')\n",
    "            doc.append(bold('F1 Score '))\n",
    "            doc.append(' = 2 (Precision * Recall)/(Precision + Recall).\\n')\n",
    "            doc.append(bold('Brier Score '))\n",
    "            doc.append(''' - mean squared error (MSE) between predicted probabilities (between 0 and 1) and the expected values (0 or 1). Brier score summarizes the magnitude of the forecasting error and takes a value between 0 and 1 (with better models having score close to 0).\\n\\n''')\n",
    "            with doc.create(Tabular('|l|l|')) as table:\n",
    "                table.add_hline()\n",
    "                table.add_row((bold('Metric'), bold('Score')))\n",
    "                table.add_hline()\n",
    "                table.add_row(('Accuracy', auc))\n",
    "                table.add_row(('Precision', precision))\n",
    "                table.add_row(('Recall', recall))\n",
    "                table.add_row(('F1 Score',f1))\n",
    "                table.add_row(('Brier Score', brier))\n",
    "                table.add_hline()\n",
    "            doc.append('\\n\\n')\n",
    "       \n",
    "\n",
    "         # plot ROC and AUC\n",
    "        with doc.create(Subsection('Reciever Operating Characteristic (ROC) and Area Under the Curve (AUC) for Test Set', numbering=0)):\n",
    "            doc.append('''The ROC curve graphically shows the trade-off between between true-positive rate and false-positive rate.The AUC summarizes the ROC curve - where the AUC is close to unity, classification is successful, while an AUC of 0.5 indicates the model is performs as well as a random guess.''')\n",
    "            with doc.create(Figure(position='h!')) as roc_curve: \n",
    "                roc_curve.add_image(roc, width='220px')  \n",
    "                doc.append(HorizontalSpace(\"2cm\"))\n",
    "                doc.append(MediumText('AUC = '+ str(auc)+'\\n\\n\\n\\n\\n\\n\\n\\n'))\n",
    "                \n",
    "                \n",
    "        # plot histogram of output values\n",
    "        with doc.create(Subsection('Histogram of the Output Probabilities for Test Set', numbering=0)):\n",
    "            with doc.create(Figure(position='h!')) as histogram:\n",
    "                histogram.add_image(hist, width='230px')\n",
    "                \n",
    "    \n",
    "        # plot 2D histogram of output values and some object parameter\n",
    "        with doc.create(Subsection('2D Histogram of the Output vs One Object Feature for Test Set', numbering=0)):\n",
    "            with doc.create(Figure(position='h!')) as histogram_2d:\n",
    "                histogram_2d.add_image(hist_2d, width='230px')\n",
    "                \n",
    "                \n",
    "    doc.generate_pdf('full', clean_tex=False)\n",
    "    return\n",
    "\n",
    "report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
